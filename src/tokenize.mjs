function createError(message, line, column) {
    const error = new Error(message);
    error.context = {};
    error.context.position = { line, column };
    return error;
}

export function tokenize(fullInput) {
    // Generated by ðŸ¤–
    // Track line and column for better error messages
    const sourceMap = new Map(); // Maps token index to {line, column}

    // First, remove multi-line comments (;...;)
    // This needs to be done before tokenizing since they can span multiple lines
    let processedInput = '';
    let i = 0;
    let inMultiLineComment = false;
    let commentStartLine = 0;
    let commentStartCol = 0;
    let currentLine = 0;
    let currentCol = 0;

    // Helper function to track position
    function updatePosition(char) {
        if (char === '\n') {
            currentLine++;
            currentCol = 0;
        } else {
            currentCol++;
        }
    }

    while (i < fullInput.length) {
        // Check for multi-line comment start
        if (!inMultiLineComment && i < fullInput.length - 1 &&
            fullInput[i] === '(' && fullInput[i + 1] === ';') {
            inMultiLineComment = true;
            commentStartLine = currentLine;
            commentStartCol = currentCol;
            i += 2; // Skip past (;
            updatePosition('(');
            updatePosition(';');
            continue;
        }

        // Check for multi-line comment end
        if (inMultiLineComment && i < fullInput.length - 1 &&
            fullInput[i] === ';' && fullInput[i + 1] === ')') {
            inMultiLineComment = false;
            i += 2; // Skip past ;)
            updatePosition(';');
            updatePosition(')');
            continue;
        }

        // Only add to processed input if not in a multi-line comment
        if (!inMultiLineComment) {
            processedInput += fullInput[i];
        }

        updatePosition(fullInput[i]);
        i++;
    }

    // Check for unclosed multi-line comment
    if (inMultiLineComment) {
        throw new createError(
            `Unclosed multi-line comment starting at line ${commentStartLine + 1}, column ${commentStartCol + 1}`,
            commentStartLine + 1,
            commentStartCol + 1)
    }

    // Split into lines again for single-line comment processing
    const filteredLines = processedInput.split('\n')
        .map(line => {
            const commentIndex = line.indexOf(';;');
            return commentIndex >= 0 ? line.substring(0, commentIndex) : line;
        });

    const input = filteredLines.join('\n');

    // Special handling for quoted strings (to preserve them during tokenization)
    const tokens = [];
    let inString = false;
    let currentToken = "";
    i = 0;

    // Reset position tracking for second pass
    currentLine = 0;
    currentCol = 0;
    let stringStartLine = 0;
    let stringStartCol = 0;
    let tokenStartLine = 0;
    let tokenStartCol = 0;

    // Special handling for memory instructions with offset/align attributes
    // This regex will match patterns like "i32.load", "i32.load8_s", etc.
    const memInstructionRegex = /i(32|64)\.(load|store)(\d+)?(_[su])?/;

    while (i < input.length) {
        const char = input[i];

        if (inString) {
            // We're inside a quoted string
            currentToken += char;
            if (char === '"' && input[i - 1] !== '\\') {
                // End of string if quote is not escaped
                tokens.push(currentToken);
                sourceMap.set(tokens.length - 1, { line: stringStartLine + 1, column: stringStartCol + 1 });
                currentToken = "";
                inString = false;
            }
        } else {
            // Not in a string
            if (char === '"') {
                // Start of a string
                if (currentToken) {
                    // Push any accumulated token
                    const splitTokens = currentToken.trim().split(/\s+/).filter(t => t);
                    for (const t of splitTokens) {
                        tokens.push(t);
                        sourceMap.set(tokens.length - 1, { line: tokenStartLine + 1, column: tokenStartCol + 1 });
                    }
                    currentToken = "";
                }
                currentToken = char;
                inString = true;
                stringStartLine = currentLine;
                stringStartCol = currentCol;
            } else if (char === '(' || char === ')') {
                // Handle parentheses as separate tokens
                if (currentToken) {
                    const splitTokens = currentToken.trim().split(/\s+/).filter(t => t);
                    for (const t of splitTokens) {
                        tokens.push(t);
                        sourceMap.set(tokens.length - 1, { line: tokenStartLine + 1, column: tokenStartCol + 1 });
                    }
                    currentToken = "";
                }
                tokens.push(char);
                sourceMap.set(tokens.length - 1, { line: currentLine + 1, column: currentCol + 1 });
            } else if (char === ' ' || char === '\t' || char === '\n' || char === '\r') {
                // Handle whitespace
                if (currentToken) {
                    // Check if we have a memory instruction
                    if (memInstructionRegex.test(currentToken)) {
                        // Look ahead for offset= or align= attributes
                        let j = i;
                        let attrToken = '';
                        let validAttribute = false;

                        while (j < input.length && input[j] !== ')' && input[j] !== '\n') {
                            attrToken += input[j];
                            if (attrToken.includes('offset=') || attrToken.includes('align=')) {
                                validAttribute = true;
                            }
                            j++;
                        }

                        // If we found attributes, combine them with the instruction
                        if (attrToken.trim() && validAttribute) {
                            // Push the combined token
                            tokens.push(currentToken + attrToken);
                            sourceMap.set(tokens.length - 1, { line: tokenStartLine + 1, column: tokenStartCol + 1 });

                            // Validate attribute format
                            const offsetMatch = attrToken.match(/offset=([^\s]+)/);
                            const alignMatch = attrToken.match(/align=([^\s]+)/);

                            if (offsetMatch && Number.isNaN(Number.parseInt(offsetMatch[1], 10))) {
                                throw new createError(
                                    `Invalid offset value "${offsetMatch[1]}" at line ${tokenStartLine + 1}, column ${tokenStartCol + 1 + currentToken.length + attrToken.indexOf('offset=')}`,
                                    tokenStartLine + 1,
                                    tokenStartCol + 1);
                            }

                            if (alignMatch && Number.isNaN(Number.parseInt(alignMatch[1], 10))) {
                                throw new createError(
                                    `Invalid align value "${alignMatch[1]}" at line ${tokenStartLine + 1}, column ${tokenStartCol + 1 + currentToken.length + attrToken.indexOf('align=')}`,
                                    tokenStartLine + 1,
                                    tokenStartCol + 1);
                            }

                            currentToken = '';
                            i = j; // Skip ahead
                            continue;
                        }
                    }

                    // Default behavior for other tokens
                    const splitTokens = currentToken.trim().split(/\s+/).filter(t => t);
                    for (const t of splitTokens) {
                        tokens.push(t);
                        sourceMap.set(tokens.length - 1, { line: tokenStartLine + 1, column: tokenStartCol + 1 });
                    }
                    currentToken = "";
                }
                // Prepare for next token
                if (char === ' ' || char === '\t') {
                    tokenStartLine = currentLine;
                    tokenStartCol = currentCol + 1; // +1 because we'll advance past this whitespace
                } else if (char === '\n') {
                    tokenStartLine = currentLine + 1; // +1 because we're moving to next line
                    tokenStartCol = 0;
                }
            } else {
                if (currentToken === "") {
                    // Start of a new token
                    tokenStartLine = currentLine;
                    tokenStartCol = currentCol;
                }
                currentToken += char;
            }
        }

        // Update position
        if (char === '\n') {
            currentLine++;
            currentCol = 0;
        } else {
            currentCol++;
        }

        i++;
    }

    // Check for unclosed string literal
    if (inString) {
        throw new createError(
            `Unclosed string literal starting at line ${stringStartLine + 1}, column ${stringStartCol + 1}`,
            stringStartLine + 1,
            stringStartCol + 1);
    }

    // Push any remaining token
    if (currentToken) {
        if (inString) {
            tokens.push(currentToken); // Push incomplete string as-is
            sourceMap.set(tokens.length - 1, { line: stringStartLine + 1, column: stringStartCol + 1 });
        } else {
            const splitTokens = currentToken.trim().split(/\s+/).filter(t => t);
            for (const t of splitTokens) {
                tokens.push(t);
                sourceMap.set(tokens.length - 1, { line: tokenStartLine + 1, column: tokenStartCol + 1 });
            }
        }
    }

    const filteredTokens = tokens.filter(token => token.length > 0);

    // Attach source map to return value
    Object.defineProperty(filteredTokens, 'sourceMap', {
        value: sourceMap,
        enumerable: false
    });

    return filteredTokens;
}